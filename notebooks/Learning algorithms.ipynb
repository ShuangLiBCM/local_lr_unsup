{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mirrored STDP implements autoencoder learning with spiking neurons\n",
    "#### Kendra Bubank, Univ. of Chicago, 2015/12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The autoencoding algorithm is implemeneted in a two-layer network. The input (visible) layer and the hidden layer, which projects back to the input layer. The learning rule is based on the difference between the activity of visible layer due to inputs, and the reconstructed activity from the hidden layer activity through the reciprocal feedback connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ \\Delta w_{ij} = \\eta(x_i - x'_i)y_j $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ",where $y_j = \\sum_i w_{ij}x_i$, and $x'_{i}= \\sum_{j}g(y_jw_{ij})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function is obtained from the squared error of the predicted output\n",
    "$\\epsilon = ||x_i - x'_i||^2 = \\sum_i (x_i - x'_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the stochastic gradient descent, we need to obtain the dependence of the error $\\epsilon$ on weight $w_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial \\epsilon}{\\partial w_{ij}} = 2\\sum_{i}(x_i-x'_i)(\\frac{\\partial x_i}{\\partial w_{ij}}-\\frac{\\partial x'_i}{\\partial w_{ij}})$ -- eq(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that the input activity $x_i$ is not dependent on the weight $w_{ij}$, $\\frac{\\partial x_i}{\\partial w_{ij}}= 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we denote $g_j'=\\frac {\\partial g_j}{\\partial w_{ij}}$,from the definition, $\\frac{\\partial x'_i}{\\partial w_{ij}} = \\sum_j [g_j'\\frac {\\partial y_j}{\\partial w_{ij}}w_{ij}+g_j'\\frac {\\partial w_{ij}}{\\partial w_{ij}}y_j]\n",
    "= g_j'\\frac {\\partial y_j}{\\partial w_{ij}}w_{ij} + g_j'y_j\\delta (w_{ij}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting back to eq(1), we will have\n",
    "\n",
    "$\\frac{\\partial \\epsilon}{\\partial w_{ij}} = 2\\sum_{i}(x_i-x'_i)[0-g_j'\\frac {\\partial y_j}{\\partial w_{ij}}w_{ij} -g_j'y_j\\delta (w_{ij})]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the equation we can see that the first term depends only on the error message of neuron $i$.Differently, the second term dependents on the errors of all the visible neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent $\n",
    "-\\frac{1}{2}\\frac{\\partial \\epsilon}{\\partial w_{ij}}= (x_i-x'_i)g_j'y_j+\\sum_{i}(x_{i}-x'_{i})g_j'(\\frac {\\partial y_j}{\\partial w_{ij}}w_{ij}) $ -- eq(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plasticity in the system includes two parts: the inter-layer synaptic strength evolve according to the mirrored STDP. The weights onto the hidden layer then applied synaptic scaling to maintain target neuronal activity.The neuron model is implemented through integrate and fire model, with only the linear inputs - firing rate relationship being important for the network training (Therefore, other more complicated neuron model should give similar learning results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs to the visible layer are image patches that have been mean-subtracted. To enbale that the neuron could encode both positive and negative value with only positive firing rates, the author incorporate the ON- and OFF- cells in the visible layers. ON cell will fire at the rate of $max(0,v_{ext})$ and the OFF cells will fire at the rate of $max(0,-V_{ext})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The firing activity of the network could be seperated input three bouts. First, neuron of the visible layer fire at a spike count $x$ given the repeated inputs.  Neurons in the hidden layer fire with spike counts $y$ after a certain delay, which is due to both the synaptic tranduction and the accumulation needed for reaching spike threshold. Eventually, neuron in the visible layer fire again due to the feedback inputs from teh hidden layer with the spike counts $x'$. To prevent the reverbarating growing of the firing activity, a constant scaling factor $\\alpha <1$ is applied so that $x'\\propto \\alpha x $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use $W$ to present the weight matrix of visible layer to hidden layer, then the tied-weights along the weak feedback will be maintained when we enforce during the learning the rule $Q = \\alpha W^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that $x'$ is $\\alpha$ the times of $x$, we need to scale $x'$ to calculate efficiently the error, which give rise to the scaled reconstruction error  $L_{spike} = ||x-\\frac{1}{\\alpha}x'||^2$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will have the updated autoencoder learning rule\n",
    "### $\\Delta w_{ij} = (x_i - \\frac{1}{\\alpha}x_i')y_i$ -- scaled autoencoder rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As neurons in the network utilize the leaky - integrate and firing models,their firing rate could be approximated by rectified linear function: $ y = max(0,Wx), x' \\approx \\alpha W^Ty $\n",
    "Putting the definition into eq(2), the first term of the gradient descent rule will become $(x'_i>0)(x_i-\\frac{1}{\\alpha}x'_i)y_j$\n",
    ", where $(x_i>0)$ is the first order derivatives of function $max(0,Wx)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this paper is then to show that the biological realistic synaptic plasticity rules used by the neurons could implement this scaled autoencoder learning rule both for feedforward and feedaback connections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mirrored STDP rule leads to learning of symmetric connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "According to previous experimental finding on the spike time dependent plasticity, the synaptic weights updating rule in the feedforward pathway could be written as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Delta w_{ij} = \\eta \\sum_{k\\in S_{pre}}\\sum_{l\\in S_{post}}\\left \\{\\begin{array}\n",
    "+e^{-|t_l-t_k|/\\tau_+}, if\\space t_l> t_k\\\\\n",
    "-e^{-|t_k-t_l|/\\tau_-}, if\\space t_l\\leq t_k\n",
    "\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ",where $\\eta$ is the learning rate,$\\tau_+$ and $\\tau_-$ are the time constant for potentiation and depression respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reversed learning rule is applied to the feedback pathway, as adopted from STDP experimental findings on synapses at distal dendrits.\n",
    "$\\Delta w_{ij} = \\eta \\sum_{k\\in S_{pre}}\\sum_{l\\in S_{post}}\\left \\{\\begin{array}\n",
    "+e^{-|t_l-t_k|/\\tau_+}, if\\space t_l< t_k\\\\\n",
    "-e^{-|t_k-t_l|/\\tau_-}, if\\space t_l\\geq t_k\n",
    "\\end{array}\n",
    "\\right.\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse coding of sensory inputs\n",
    "#### Bruno Olshausen and David Field, 2004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From this paper, we need to establish four key understanding of sparse coding\n",
    "#### 1. It increases the information storage capacity.\n",
    "#### 2. It makes the structure in natural stimulus explicit.\n",
    "#### 3. It represents complex data in a way that is easier to read out at subsequent levels of processing.\n",
    "#### 4. It saves energy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory of sparse coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early work on associative memory models show that sparse representations are most effective for storing patterns, as they maximize memory capacity due to fewer collisions among different patterns. Later work also show that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
